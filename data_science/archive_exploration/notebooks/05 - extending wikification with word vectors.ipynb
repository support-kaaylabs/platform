{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "plt.rcParams['figure.figsize'] = (30, 30)\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/calm_records.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get average word vector for our collection-level record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = df.loc[269229]['AdminHistory'][0]\n",
    "soup = BeautifulSoup(record, 'html.parser')\n",
    "plain_text = soup.get_text()\n",
    "plain_text = ' '.join(plain_text.split())\n",
    "\n",
    "\n",
    "print(plain_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(plain_text)\n",
    "doc_avg_wv = np.array([word.vector for word in doc]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get average word vector for a specific, known wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = wikipedia.page('Hinge_loss')\n",
    "doc = nlp(h.content)\n",
    "np.array([word.vector for word in doc]).mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_page = nlp(wikipedia.page('dog').content)\n",
    "known_avg_wv = np.array([word.vector for word in known_page]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get average word vectors for all pages in wikipedia search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.search('pool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_avg_wvs = pd.Series()\n",
    "possible_page_titles = [title for title in wikipedia.search('chicken')\n",
    "                        if 'disambiguation' not in title]\n",
    "print(possible_page_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(page_name):\n",
    "    pages = []\n",
    "    try:\n",
    "        wikipedia.page(page_name).url\n",
    "        # we're just hitting the url to check for disambiguation errors\n",
    "        pages.append(page_name)\n",
    "    except wikipedia.exceptions.DisambiguationError as disambiguation:\n",
    "        for option in disambiguation.options:\n",
    "            # Note that we're only going one level deep into disambiguations.\n",
    "            # This should be more than enough for our purposes, and it's easy\n",
    "            # to get caught in horrible, endless loops and branches if we make\n",
    "            # this properly recursive.\n",
    "            try: \n",
    "                wikipedia.page(option).url\n",
    "                pages.append(option)\n",
    "            except wikipedia.exceptions.DisambiguationError: pass\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    return [j for i in list_of_lists for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_page_names = np.unique(flatten([get_pages(page_name) \n",
    "                                    for page_name in possible_page_titles]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_page = nlp(wikipedia.page('dog').summary)\n",
    "known_avg_wv = np.array([word.vector for word in known_page]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_wvs = pd.Series({page_name: (np.array([word.vector for word in \n",
    "                                                 nlp(wikipedia.page(page_name).content)])\n",
    "                                       .mean(axis=0))\n",
    "                           for page_name in tqdm(all_page_names)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in our example we have a load of text about swimming, and we want to figure out which version of the page 'pool' is most relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = candidate_wvs.to_frame('word vector')\n",
    "\n",
    "df['similarity'] = df['word vector'].apply(lambda avg_wv: cosine(avg_wv, known_avg_wv))\n",
    "df.sort_values(by='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.page('Chicken wings as food').url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in nlp(plain_text).ents:\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_page_names = np.unique(flatten([get_pages(page_name) \n",
    "                                    for page_name in wikipedia.search(str(nlp(plain_text).ents[0]))]))\n",
    "\n",
    "candidate_wvs = pd.Series({page_name: (np.array([word.vector for word in \n",
    "                                                 nlp(wikipedia.page(page_name).content)])\n",
    "                                       .mean(axis=0))\n",
    "                           for page_name in tqdm(all_page_names)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = candidate_wvs.to_frame('word vector')\n",
    "\n",
    "df['similarity'] = df['word vector'].apply(lambda avg_wv: cosine(avg_wv, doc_avg_wv))\n",
    "df.sort_values(by='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.search('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_2 = wikipedia.page('chicken').summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try (ratio'd) set intersection of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/pimh/Downloads/glove.6B/glove.6B.300d.txt') as f:\n",
    "    glove = {}\n",
    "    for line in tqdm(f.read().split('\\n')):\n",
    "        try:\n",
    "            line = line.split()\n",
    "            id = line[0]\n",
    "            wv = np.array(line[1:]).astype(np.float32)\n",
    "            glove[id] = wv\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.page('swimming').summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_page_summaries = pd.Series({page: wikipedia.page(page).content.lower()\n",
    "                                      for page in tqdm(get_pages('amazon'))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_summary = wikipedia.page('rainforest').content.lower()\n",
    "known_wv = get_doc_vector(known_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vector(input_text):\n",
    "    return np.stack([glove[str(word)] for word in nlp(input_text)\n",
    "                     if str(word) in glove.keys()]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = candidate_page_summaries.apply(get_doc_vector).to_frame('wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['similarity'] = frame['wv'].apply(lambda x: cosine(x, known_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.sort_values(by='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
